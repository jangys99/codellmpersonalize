{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation script to annotate collected rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openai\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE PARAMETERS START --------------\n",
    "# Define the experiment ID\n",
    "logs_id = 'demo_test_v3'\n",
    "experiment_id = 'bt_7_p1_train_pair_test_v3' # Replace with your actual experiment ID\n",
    "FINETUNE_MODEL = \"gpt-3.5-turbo-1106\"\n",
    "SUFFIX = f\"bt_7_p1_v3\"\n",
    "FLAG_FILTER = True # if true, filter out episodes with failed steps, if false, keep all demonstrations as is\n",
    "STARTING_EPISODE = 25\n",
    "NUM_EPISODES_PER_SCENE = 22\n",
    "# CONFIGURE PARAMETERS END --------------\n",
    "\n",
    "# Replace 'experiment_log.json' with the path to your actual JSON file\n",
    "ROOT_PATH = pathlib.Path(\"__file__\").resolve().parent.parent\n",
    "EXP_FOLDER = os.path.join(ROOT_PATH, \"experiments\")\n",
    "\n",
    "OUTPUT_PATH = os.path.join(EXP_FOLDER, experiment_id)\n",
    "LOGS_FOLDER = os.path.join(ROOT_PATH, \"logs\")\n",
    "CONFIGS_FOLDER = os.path.join(ROOT_PATH, \"cos_eor\", \"configs\", \"local\")\n",
    "ENVS_FILE_PATH = os.path.join(CONFIGS_FOLDER , \"envs_demo.yaml\")\n",
    "\n",
    "# Note: put the OpenAI key here:\n",
    "with open(os.path.join(CONFIGS_FOLDER, \"api_key.yaml\")) as kfile:\n",
    "    k = yaml.safe_load(kfile)\n",
    "openai.api_key = k['key'] # PUT THE API_KEY into key.txt file\n",
    "if 'organization' in k:\n",
    "    openai.organization = k['organization']\n",
    "\n",
    "TRAIN_FILE_NAME_OUTPUT = \"train.jsonl\"\n",
    "VALID_FILE_NAME_OUTPUT = \"valid.jsonl\"\n",
    "TEST_FILE_NAME_OUTPUT = \"test.jsonl\"\n",
    "META_FILE_NAME = \"info.yaml\"\n",
    "\n",
    "# Constants\n",
    "ANNOTATION = \"annotation\"\n",
    "EPISODE = \"episode\"\n",
    "DIFF_CORRECT_LOC = \"diff_correct_loc\"\n",
    "EXPERIMENT = \"experiment\"\n",
    "FLAG = \"flag\"\n",
    "FINETUNE_MSG = \"finetune_message\"\n",
    "NUM_OBJECTS_DISCOVERED = \"num_objects_discovered\"\n",
    "NUM_RECS_DISCOVERED = \"num_recs_discovered\"\n",
    "MISSION_COMPLETE = \"mission complete\"\n",
    "OUTCOME = \"outcome\"\n",
    "PROMPT = \"prompt\"\n",
    "REWARD = \"reward\"\n",
    "REWARD_WEIGHTS = {NUM_OBJECTS_DISCOVERED: 1, NUM_RECS_DISCOVERED: 1, DIFF_CORRECT_LOC: 10}\n",
    "SCENE = \"scene\"\n",
    "SUC = \"succeeded\"\n",
    "FAIL = \"failed\"\n",
    "SKIP = \"skipped\"\n",
    "SUC_STEPS = \"successful_steps\"\n",
    "\n",
    "# Read the scene IDs from the envs.yaml file\n",
    "with open(ENVS_FILE_PATH , 'r') as file:\n",
    "    scenes = yaml.safe_load(file).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_paths ['/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-05.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-12.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-22.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-31.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-42.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-49.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-41-56.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-10.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-17.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-23.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-29.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-39.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-47.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-42-57.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-05.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-11.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-18.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-27.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-41.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-43-53.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-01.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-10.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-20.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-26.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-37.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-47.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-44-59.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-45-11.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-45-21.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-45-31.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-45-43.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-45-49.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-04.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-10.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-23.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-34.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-40.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-46-53.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-03.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-13.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-20.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-28.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-36.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-44.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-47-52.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-03.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-16.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-26.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-35.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-43.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-51.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-48-59.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-06.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-20.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-27.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-36.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-45.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-49-55.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-01.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-08.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-17.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-25.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-32.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-39.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-47.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-50-55.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-04.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-13.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-22.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-30.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-39.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-49.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-51-56.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-52-04.json', '/workspace/codellmpersonalize/logs/demo_test_v3/demo/pomaria_1_int/data_2026-01-06_08-52-13.json']\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "successful episodes {'pomaria_1_int': [26, 27, 28, 30, 31, 34, 35, 36, 37, 39, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 54, 56]}\n"
     ]
    }
   ],
   "source": [
    "# Load the scene IDs from the envs.yaml file\n",
    "def load_scenes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Function to get log file paths for a given experiment ID and scene ID\n",
    "def get_experiment_log_paths(experiment_id, scene_id):\n",
    "    logs_path_pattern = f'{LOGS_FOLDER}/{experiment_id}/demo/{scene_id}/data_*.json'\n",
    "    all_paths = sorted(glob.glob(logs_path_pattern), reverse=False)[STARTING_EPISODE:]\n",
    "    print ('all_paths', all_paths)\n",
    "    return all_paths\n",
    "\n",
    "def reward(result):\n",
    "    reward = sum(result[k] * REWARD_WEIGHTS[k] for k in REWARD_WEIGHTS)\n",
    "    return reward\n",
    "\n",
    "def compile_steps(steps):\n",
    "    # Enumerate over the steps, starting at 1, and format them into a string\n",
    "    nl = \"\\n\".join(f\"step {index}: {step}\" for index, step in enumerate(steps, start=1))\n",
    "    nl_without_prefix = nl.replace('step 1: ', '')\n",
    "    return nl_without_prefix\n",
    "\n",
    "def prepare_example_conversation(system_msg, user_msg, assistant_message):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_msg,})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def annotate_record(record, experiment_id, scene_id, episode_id):\n",
    "    result = {}\n",
    "    result[SCENE] = scene_id\n",
    "    result[EXPERIMENT] = experiment_id\n",
    "    result[EPISODE] = episode_id\n",
    "    result[NUM_OBJECTS_DISCOVERED] = len(record[OUTCOME][\"objects_discovered\"])\n",
    "    result[NUM_RECS_DISCOVERED] = len(record[OUTCOME][\"recs_discovered\"])\n",
    "    result[DIFF_CORRECT_LOC] = record[OUTCOME][\"count_correct\"][\"end\"] - record[OUTCOME][\"count_correct\"][\"start\"]\n",
    "    # craft a response based on successful steps\n",
    "    result[\"all_steps\"] = [l['step_raw'] for l in record['logs']]\n",
    "    result[SUC_STEPS] = [l['step_raw'] for l in record[\"logs\"] if l[FLAG] == SUC]\n",
    "    system_msg = record[\"low_level\"][\"prompt\"][\"system\"]\n",
    "    user_msg = record[\"low_level\"][\"prompt\"][\"user\"] + \"\\nstep 1: \" \n",
    "    assistant_msg = compile_steps(result[\"all_steps\"]) # result[SUC_STEPS])\n",
    "    result[FINETUNE_MSG] = prepare_example_conversation(system_msg=system_msg, user_msg=user_msg, assistant_message=assistant_msg)\n",
    "    result[REWARD] = reward(result)\n",
    "    return result\n",
    "\n",
    "def annotate_episode(episode, experiment_id, scene_id):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Function to load experiment logs and add the scene name\n",
    "def load_and_annotate_logs(experiment_id, scenes):\n",
    "    all_records = []\n",
    "    all_episodes = []\n",
    "    suc_episode_ids = {}\n",
    "    for scene_id in scenes:\n",
    "        suc_episode_ids[scene_id] = []\n",
    "        log_file_paths = get_experiment_log_paths(experiment_id, scene_id)\n",
    "        for i, log_file_path in enumerate(log_file_paths):\n",
    "            if len(suc_episode_ids[scene_id]) >= NUM_EPISODES_PER_SCENE:\n",
    "                break\n",
    "            episode_id = i + STARTING_EPISODE\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                records = json.load(file)\n",
    "                # Annotate each record with the scene name\n",
    "                for record in records:\n",
    "                    record[ANNOTATION] = annotate_record(record, experiment_id, scene_id, episode_id)\n",
    "                all_records.extend(records)\n",
    "                # append an episode only if it doesn't contain failed steps\n",
    "                filter_out_episode = False\n",
    "                for record in records:\n",
    "                    failed_logs = [1 for l in record['logs'] if l[FLAG] == FAIL]\n",
    "                    skipped_logs = [l for l in record['logs'] if l[FLAG] == SKIP and l['step_raw'] != MISSION_COMPLETE]\n",
    "                    if FLAG_FILTER and len(failed_logs + skipped_logs) > 0:\n",
    "                        filter_out_episode = True\n",
    "                        print ('failed log')\n",
    "                        break\n",
    "                if not filter_out_episode:\n",
    "                    suc_episode_ids[scene_id].append(episode_id)\n",
    "                    all_episodes.append(records)\n",
    "                # annotated_episode = annotate_episode(records, experiment_id, scene_id)\n",
    "    return all_records, all_episodes, suc_episode_ids\n",
    "\n",
    "# Load and annotate logs\n",
    "annotated_logs, annotated_episodes, suc_episode_ids = load_and_annotate_logs(logs_id, scenes)\n",
    "print ('successful episodes', suc_episode_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pomaria_1_int': 96})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_records = [log for log in annotated_logs if log['annotation']['diff_correct_loc'] > 0]\n",
    "pick_records = [log for log in annotated_logs if log['annotation']]\n",
    "\n",
    "from collections import Counter\n",
    "Counter([log[ANNOTATION][SCENE] for log in positive_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_episode_steps = []\n",
    "for ie, episode in enumerate(annotated_episodes):\n",
    "    flag_correct = False\n",
    "    for ir, record in enumerate(episode):\n",
    "        if record[ANNOTATION]['diff_correct_loc'] > 0:\n",
    "            correct_episode_steps.append((ie, ir)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct step episodes 70\n",
      "[(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (4, 1), (4, 2), (4, 3), (5, 1), (6, 1), (6, 2), (7, 1), (7, 2), (7, 3), (7, 4), (8, 1), (8, 2), (8, 3), (9, 1), (9, 2), (9, 3), (10, 1), (10, 2), (11, 1), (11, 2), (12, 1), (12, 2), (12, 3), (12, 4), (13, 1), (13, 2), (13, 3), (14, 1), (14, 2), (14, 3), (15, 1), (15, 2), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (17, 1), (17, 2), (17, 3), (17, 4), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (19, 1), (19, 2), (19, 3), (19, 4), (20, 1), (20, 2), (20, 3), (20, 4), (21, 1), (21, 2)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print ('Number of correct step episodes', len(correct_episode_steps))\n",
    "print (correct_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats to log: the objects across seeds within the same scene\n",
    "# success rates of the generated plans\n",
    "# receptacle counts\n",
    "from collections import defaultdict\n",
    "object_stats = defaultdict(list) # scene, seed -> object lists\n",
    "msgs_counts = defaultdict(list)\n",
    "gt_correct_counts = defaultdict(list) # successful if all wrong objects cleared\n",
    "mission_complete_counts = defaultdict(list) # episode ends with mission complete message\n",
    "\n",
    "correct_counts = defaultdict(list)\n",
    "for ie, episode in enumerate(annotated_episodes):\n",
    "    scene = episode[0][ANNOTATION][SCENE]\n",
    "    objects = list(episode[0]['logs'][0]['current_mapping']['start'].keys())\n",
    "    msgs_counts[scene].append(len(episode))\n",
    "    object_stats[scene].append(objects)\n",
    "    diff = episode[-1][OUTCOME][\"count_correct\"]['end'] - episode[0][OUTCOME]['count_correct']['start']\n",
    "    gt_diff = episode[-1][OUTCOME]['count_correct']['end'] + episode[-1][OUTCOME]['count_wrong']['end'] - episode[0][OUTCOME]['count_correct']['start']\n",
    "    gt_correct_counts[scene].append(gt_diff)\n",
    "    correct_counts[scene].append(diff)\n",
    "    last_msg_mission_complete = episode[-1]['logs'][-1]['step_raw'] == MISSION_COMPLETE\n",
    "    if last_msg_mission_complete:\n",
    "        mission_complete_counts[scene].append(1)\n",
    "    else:\n",
    "        mission_complete_counts[scene].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "79 unique objects 8.545454545454545 objects per episode on average of  22 episodes\n",
      "all unique object count 79\n"
     ]
    }
   ],
   "source": [
    "# object counts\n",
    "all_unique_objects = set()\n",
    "for scene in object_stats:\n",
    "    print (scene)\n",
    "    unique_objects = set()\n",
    "    all_objects_count = []\n",
    "    for objects in object_stats[scene]:\n",
    "        unique_objects = unique_objects.union(objects)\n",
    "        all_unique_objects = all_unique_objects.union(objects)\n",
    "        all_objects_count.append(len(objects))\n",
    "    print (len(unique_objects), 'unique objects', sum(all_objects_count)/len(all_objects_count), 'objects per episode on average of ', len(all_objects_count), 'episodes')\n",
    "print ('all unique object count', len(all_unique_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "average 5.181818181818182 messages per episode\n",
      "In total 114 messages.\n"
     ]
    }
   ],
   "source": [
    "# number of messages counts\n",
    "total_msgs = 0\n",
    "for scene in msgs_counts:\n",
    "    print (scene)\n",
    "    total_msgs += sum(msgs_counts[scene])\n",
    "    print ('average', sum(msgs_counts[scene])/len(msgs_counts[scene]), 'messages per episode')\n",
    "print ('In total', total_msgs, 'messages.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "[Actual] ~ 3.1818181818181817 diff correct objects per episode\n",
      "[Ground Truth] ~ 3.909090909090909 diff correct objects per episode\n",
      "[Ratio] success rate 81.3953488372093%\n",
      "[mission complete] 22 out of 22 | rate = 100.0%\n"
     ]
    }
   ],
   "source": [
    "# diff rates and success rates (total number of diff)\n",
    "for scene in correct_counts:\n",
    "    print (scene)\n",
    "    print ('[Actual] ~', sum(correct_counts[scene])/len(correct_counts[scene]), 'diff correct objects per episode')\n",
    "    print ('[Ground Truth] ~', sum(gt_correct_counts[scene])/len(gt_correct_counts[scene]), 'diff correct objects per episode')\n",
    "    suc_rate = sum(correct_counts[scene])/sum(gt_correct_counts[scene]) * 100\n",
    "    print ('[Ratio] success rate', f'{suc_rate}%' )\n",
    "    num_mission_complete = sum(mission_complete_counts[scene])\n",
    "    num_all_episodes = len(mission_complete_counts[scene])\n",
    "    mission_complete_rate = num_mission_complete/num_all_episodes * 100\n",
    "    print (f'[{MISSION_COMPLETE}] {num_mission_complete} out of {num_all_episodes} | rate = {mission_complete_rate}%')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to write messages to jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove forward to write messages to jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to write messages to jsonl"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to write messages to jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPISODES = 20\n",
    "NUM_VALID_EPISODES = 2\n",
    "\n",
    "training_data = []\n",
    "validation_data = []\n",
    "episode_counter = {}\n",
    "for episode in annotated_episodes:\n",
    "    scene = episode[0][ANNOTATION][SCENE]\n",
    "    if scene in episode_counter:\n",
    "        episode_counter[scene] += 1\n",
    "    else:\n",
    "        episode_counter[scene] = 1\n",
    "    if episode_counter[scene] > NUM_TRAIN_EPISODES:\n",
    "        for log in episode:\n",
    "            validation_data.append(log[ANNOTATION][FINETUNE_MSG])\n",
    "    else:\n",
    "        for log in episode:\n",
    "            training_data.append(log[ANNOTATION][FINETUNE_MSG])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print (len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "            \n",
    "def write_metadata_file(filepath):\n",
    "    with open(filepath, 'w') as out:\n",
    "        flag = {}\n",
    "        flag['logs_id'] = logs_id\n",
    "        flag['scenes'] = scenes\n",
    "        flag['starting_episode'] = STARTING_EPISODE\n",
    "        flag['num_episodes_per_scene'] = NUM_EPISODES_PER_SCENE\n",
    "        flag['chosen_episodes_per_scene'] = suc_episode_ids\n",
    "        flag['num_train_episodes'] = NUM_TRAIN_EPISODES\n",
    "        flag['num_valid_episodes'] = NUM_VALID_EPISODES\n",
    "        flag['num_train_samples'] = len(training_data)\n",
    "        flag['num_valid_samples'] = len(validation_data) \n",
    "        yaml.dump(flag, out, default_flow_style=False)\n",
    "        print (flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logs_id': 'demo_test_v3', 'scenes': ['pomaria_1_int'], 'starting_episode': 25, 'num_episodes_per_scene': 22, 'chosen_episodes_per_scene': {'pomaria_1_int': [26, 27, 28, 30, 31, 34, 35, 36, 37, 39, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 54, 56]}, 'num_train_episodes': 20, 'num_valid_episodes': 2, 'num_train_samples': 104, 'num_valid_samples': 10}\n"
     ]
    }
   ],
   "source": [
    "training_file_name = os.path.join(OUTPUT_PATH, TRAIN_FILE_NAME_OUTPUT)\n",
    "# Create the parent directory if it doesn't exist\n",
    "Path(training_file_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "if NUM_VALID_EPISODES > 0:\n",
    "    validation_file_name = os.path.join(OUTPUT_PATH, VALID_FILE_NAME_OUTPUT)\n",
    "    write_jsonl(validation_data, validation_file_name)\n",
    "\n",
    "write_metadata_file(os.path.join(OUTPUT_PATH, META_FILE_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to create finetune jobs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove forward to create finetune jobs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to create finetune jobs"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to create finetune jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetune_upload_response():\n",
    "    training_response = openai.File.create(file=open(training_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "    training_file_id = training_response[\"id\"]\n",
    "    print(\"Training file ID:\", training_file_id)\n",
    "    if NUM_VALID_EPISODES > 0:\n",
    "        validation_response = openai.File.create(file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "        validation_file_id = validation_response[\"id\"]\n",
    "        print(\"Validation file ID:\", validation_file_id)\n",
    "        return {\"training_response\": training_response, \"validation_response\": validation_response}\n",
    "    else:\n",
    "        return {\"training_response\": training_response}\n",
    "\n",
    "\n",
    "def create_finetune_response_and_log(training_file_id, validation_file_id=None):\n",
    "    if validation_file_id is not None:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, validation_file=validation_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, hyperparameters={\"n_epochs\":1})\n",
    "    else:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, hyperparameters={\"n_epochs\":1})\n",
    "    job_id = response[\"id\"]\n",
    "    with open(os.path.join(OUTPUT_PATH, \"job_info.txt\"), 'w') as out:\n",
    "        flag = \"\"\n",
    "        flag += f\"training file id: {training_file_id}\\n\"\n",
    "        if validation_file_id is not None:\n",
    "            flag += f\"validation file id: {validation_file_id}\\n\"\n",
    "        flag += f\"finetune job id: {job_id}\\n\"\n",
    "        flag += f\"finetune model: {FINETUNE_MODEL}\\n\"\n",
    "        flag += f\"finetune suffix: {SUFFIX}\"\n",
    "        out.write(flag)\n",
    "        print(\"Status:\", response[\"status\"])\n",
    "        print(\"Job ID:\", response[\"id\"])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-SehoY1bk5REz4sypdMHFAv\n",
      "Validation file ID: file-D34BQdwdpdryRA5tDWK1nw\n",
      "Status: validating_files\n",
      "Job ID: ftjob-GrqKUBulEtCOE0aAJvwbiHO3\n"
     ]
    }
   ],
   "source": [
    "upload_result = create_finetune_upload_response()\n",
    "training_response = upload_result['training_response']\n",
    "validation_file_id = None\n",
    "training_file_id = training_response['id']\n",
    "if NUM_VALID_EPISODES > 0:\n",
    "    validation_response = upload_result['validation_response']\n",
    "    validation_file_id = validation_response['id']\n",
    "finetune_response = create_finetune_response_and_log(training_file_id, validation_file_id)\n",
    "job_id = finetune_response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-GrqKUBulEtCOE0aAJvwbiHO3\n",
      "Status: validating_files\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])\n",
    "print(\"Trained Tokens:\", response[\"trained_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaelic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fde01e40ce835f40519108abea24a46644765d2b0dbe10643cc34bafc8aa5e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
